{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcda241a",
   "metadata": {},
   "source": [
    "ODIR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0253ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING SET ANNOTATION PROCESSING\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_excel(r\"Training Set\\Annotation\\training annotation (English).xlsx\")\n",
    "classes = ['N','D','G','C','A','H','M','O']\n",
    "df[classes] = df[classes].astype(int)\n",
    "df['labels'] = df[classes].values.tolist()\n",
    "df[['ID', 'labels']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18736b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ONSITE TEST SET ANNOTATION PROCESSING\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_excel(r\"On-site Test Set\\Annotation\\on-site test annotation (English).xlsx\")\n",
    "classes = ['N','D','G','C','A','H','M','O']\n",
    "df[classes] = df[classes].astype(int)\n",
    "df['labels'] = df[classes].values.tolist()\n",
    "df[['ID', 'labels']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d3c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OFFSITE TEST SET ANNOTATION PROCESSING\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_excel(r\"Off-site Test Set\\Annotation\\off-site test annotation (English).xlsx\")\n",
    "classes = ['N','D','G','C','A','H','M','O']\n",
    "df[classes] = df[classes].astype(int)\n",
    "df['labels'] = df[classes].values.tolist()\n",
    "df[['ID', 'labels']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac91880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def apply_clahe(img):\n",
    "    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    cl = clahe.apply(l)\n",
    "\n",
    "    merged = cv2.merge((cl,a,b))\n",
    "    return cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6437bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomRotation(15),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((224,224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65f4beba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total images in training set -  7000\n",
      "total images in on-site test set -  2000\n",
      "total images in off-site test set -  1000\n",
      "total images in resized images -  7000\n",
      "total images in CLAHE resized images -  7000\n",
      "total images in Augmented CLAHE resized images -  10500\n"
     ]
    }
   ],
   "source": [
    "#TOTAL NUMBER OF IMAGES IN TRAINING, ON SITE AND OFF SITE TEST SETS\n",
    "\n",
    "import os\n",
    "\n",
    "image_dir_1 = r\"Training Set\\Images\"\n",
    "total_images_1 = len([f for f in os.listdir(image_dir_1) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "print(\"total images in training set - \",total_images_1)\n",
    "\n",
    "image_dir_2 = r\"On-site Test Set\\Images\"\n",
    "total_images_2 = len([f for f in os.listdir(image_dir_2) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "print(\"total images in on-site test set - \",total_images_2)\n",
    "\n",
    "image_dir_3 = r\"Off-site Test Set\\Images\"\n",
    "total_images_3 = len([f for f in os.listdir(image_dir_3) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "print(\"total images in off-site test set - \",total_images_3)\n",
    "\n",
    "image_dir_4 = r\"Resized Images\"\n",
    "total_images_4 = len([f for f in os.listdir(image_dir_4) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "print(\"total images in resized images - \",total_images_4)\n",
    "\n",
    "image_dir_5 = r\"CLAHE_Resized_Images\"\n",
    "total_images_5 = len([f for f in os.listdir(image_dir_5) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "print(\"total images in CLAHE resized images - \",total_images_5)\n",
    "\n",
    "image_dir_6 = r\"Augmented_CLAHE_Resized_Images\"\n",
    "total_images_6 = len([f for f in os.listdir(image_dir_6) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "print(\"total images in Augmented CLAHE resized images - \",total_images_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b5a8c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Analyzing 7000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7000/7000 [01:34<00:00, 73.86it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "       DATASET SIZE REPORT       \n",
      "========================================\n",
      "Total Images Scanned: 7000\n",
      "Unique Sizes Found:   72\n",
      "Average Size:         1655 x 1264\n",
      "----------------------------------------\n",
      "TOP 5 MOST COMMON SIZES:\n",
      "  Size: (512, 512)  -> Found 2873 times\n",
      "  Size: (2592, 1728)  -> Found 1408 times\n",
      "  Size: (2048, 1536)  -> Found 352 times\n",
      "  Size: (1956, 1934)  -> Found 216 times\n",
      "  Size: (2304, 1728)  -> Found 206 times\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Make sure this points to your Training Set/Images folder\n",
    "INPUT_DIR = r\"Training Set\\Images\"\n",
    "\n",
    "def analyze_sizes(folder_path):\n",
    "    # Store all sizes here as tuples: (width, height)\n",
    "    all_sizes = []\n",
    "    \n",
    "    file_list = [f for f in os.listdir(folder_path) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "    print(f\"ðŸ“Š Analyzing {len(file_list)} images...\")\n",
    "    \n",
    "    for filename in tqdm(file_list):\n",
    "        try:\n",
    "            with Image.open(os.path.join(folder_path, filename)) as img:\n",
    "                all_sizes.append(img.size) # img.size is (width, height)\n",
    "        except Exception:\n",
    "            pass # Skip broken files\n",
    "\n",
    "    # 1. Find the Most Common Size (Mode)\n",
    "    size_counts = Counter(all_sizes)\n",
    "    most_common = size_counts.most_common(5) # Get top 5\n",
    "    \n",
    "    # 2. Find Average Size\n",
    "    if all_sizes:\n",
    "        avg_w = sum(w for w, h in all_sizes) / len(all_sizes)\n",
    "        avg_h = sum(h for w, h in all_sizes) / len(all_sizes)\n",
    "    else:\n",
    "        avg_w, avg_h = 0, 0\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"       DATASET SIZE REPORT       \")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Total Images Scanned: {len(all_sizes)}\")\n",
    "    print(f\"Unique Sizes Found:   {len(size_counts)}\")\n",
    "    print(f\"Average Size:         {int(avg_w)} x {int(avg_h)}\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"TOP 5 MOST COMMON SIZES:\")\n",
    "    for size, count in most_common:\n",
    "        print(f\"  Size: {size}  -> Found {count} times\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "# Run it\n",
    "analyze_sizes(INPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e7abf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Processing 7000 images to 512x512...\n",
      "â˜• This will take a while (approx 5-10 mins due to high resolution)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7000/7000 [03:10<00:00, 36.68it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… High-Res (512px) Dataset saved to: Resized Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Your Input Path (from the kagglehub download)\n",
    "INPUT_DIR = r\"Training Set\\Images\"\n",
    "\n",
    "# New Output Path for the \"Giant\" images\n",
    "OUTPUT_DIR = r\"Resized Images\" \n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# âœ… TARGET SIZE: 2048\n",
    "TARGET_SIZE = 512\n",
    "\n",
    "def resize_pad_high_quality(image, target_size):\n",
    "    old_size = image.shape[:2] # (height, width)\n",
    "    \n",
    "    # 1. Calc Ratio\n",
    "    ratio = float(target_size) / max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    \n",
    "    # 2. Resize\n",
    "    # We use INTER_LANCZOS4 because it preserves the most detail for large images\n",
    "    image = cv2.resize(image, (new_size[1], new_size[0]), interpolation=cv2.INTER_LANCZOS4)\n",
    "    \n",
    "    # 3. Add Padding (Black Borders)\n",
    "    delta_w = target_size - new_size[1]\n",
    "    delta_h = target_size - new_size[0]\n",
    "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "    \n",
    "    # borderType=BORDER_CONSTANT with value 0 (Black)\n",
    "    new_image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=[0, 0, 0])\n",
    "    \n",
    "    return new_image\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "image_files = [f for f in os.listdir(INPUT_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "print(f\"ðŸš€ Processing {len(image_files)} images to {TARGET_SIZE}x{TARGET_SIZE}...\")\n",
    "print(\"â˜• This will take a while (approx 5-10 mins due to high resolution)...\")\n",
    "\n",
    "for filename in tqdm(image_files):\n",
    "    try:\n",
    "        img_path = os.path.join(INPUT_DIR, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        if img is None: continue\n",
    "\n",
    "        # Resize\n",
    "        final_img = resize_pad_high_quality(img, TARGET_SIZE)\n",
    "        \n",
    "        # Save\n",
    "        # We use high quality JPEG compression (95) to avoid artifacts\n",
    "        save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        cv2.imwrite(save_path, final_img, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error on {filename}: {e}\")\n",
    "\n",
    "print(f\"\\nâœ… High-Res (512px) Dataset saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13dc91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CLAHE to 7000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7000/7000 [01:10<00:00, 99.15it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLAHE-enhanced images saved to - CLAHE_Resized_Images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Directory for CLAHE-enhanced images\n",
    "CLAHE_DIR = r\"CLAHE_Resized_Images\"\n",
    "os.makedirs(CLAHE_DIR, exist_ok=True)\n",
    "\n",
    "# Get list of resized images\n",
    "resized_files = [f for f in os.listdir(OUTPUT_DIR) if f.lower().endswith(('.jpg', '.png', '.jpeg'))]\n",
    "\n",
    "print(f\"Applying CLAHE to {len(resized_files)} images...\")\n",
    "\n",
    "for filename in tqdm(resized_files):\n",
    "    img_path = os.path.join(OUTPUT_DIR, filename)\n",
    "    img = cv2.imread(img_path)\n",
    "    \n",
    "    if img is not None:\n",
    "        enhanced_img = apply_clahe(img)\n",
    "        save_path = os.path.join(CLAHE_DIR, filename)\n",
    "        cv2.imwrite(save_path, enhanced_img)\n",
    "\n",
    "print(f\"CLAHE-enhanced images saved to - {CLAHE_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af0fa666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Reload training set annotation to overwrite previous df\n",
    "df = pd.read_excel(r\"Training Set\\Annotation\\training annotation (English).xlsx\")\n",
    "classes = ['N','D','G','C','A','H','M','O']\n",
    "df[classes] = df[classes].astype(int)\n",
    "df['labels'] = df[classes].values.tolist()\n",
    "\n",
    "class ODIRDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_name = row['Left-Fundus']\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        labels = torch.tensor(row['labels'], dtype=torch.float32)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "# Create training dataset with augmentation and normalization applied via train_transform\n",
    "train_dataset = ODIRDataset(df, CLAHE_DIR, train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e236899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting 3500 images with 3 variations each...\n",
      "Augmented images saved to: Augmented_CLAHE_Resized_Images\n",
      "Total images created: 10500\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Directory for augmented and normalized images\n",
    "augmented_dir = r\"Augmented_CLAHE_Resized_Images\"\n",
    "os.makedirs(augmented_dir, exist_ok=True)\n",
    "\n",
    "# Denormalization parameters (inverse of the normalization in transforms)\n",
    "mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "\n",
    "# Number of augmentations per original image\n",
    "num_augs = 3\n",
    "\n",
    "print(f\"Augmenting {len(train_dataset)} images with {num_augs} variations each...\")\n",
    "\n",
    "for idx in range(len(train_dataset)):\n",
    "    for aug in range(num_augs):\n",
    "        image, label = train_dataset[idx]  # This applies random augmentation each time\n",
    "        \n",
    "        # Denormalize the image to bring it back to 0-1 range for saving\n",
    "        image = image * std + mean\n",
    "        image = torch.clamp(image, 0, 1)  # Ensure values are in [0, 1]\n",
    "        \n",
    "        # Save the augmented image\n",
    "        save_path = os.path.join(augmented_dir, f\"{idx}_{aug}.png\")\n",
    "        save_image(image, save_path)\n",
    "\n",
    "print(f\"Augmented images saved to: {augmented_dir}\")\n",
    "print(f\"Total images created: {len(train_dataset) * num_augs}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
